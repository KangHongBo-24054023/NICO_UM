#!/usr/bin/env python

import numpy
import cv2
from _nicoemotionrecognition_internal import modelLoader, modelDictionary, imageProcessingUtil, GUIController
import sys
import tensorflow as tf
from nicovision.VideoDevice import VideoDevice
import logging

from nicomotion import Motion
from os.path import dirname, abspath

class EmotionRecognition:
    def __init__(self, device='', robot=None, face=None, faceDetectionDelta = 10):
        """
        Initialises the EmotionRecognition

        :param device: Target video capture unit
        :type device: str
        :param faceDetectionDelta: Number of frames until face detection is refreshed
        :type faceDetectionDelta: int
        """
        self._finalImageSize = (1024,768) # Size of the final image generated by the demo
        self._categoricalInitialPosition = 260 # Initial position for adding the categorical graph in the final image
        self._faceSize = (64,64) # Input size for both models: categorical and dimensional
        self._deviceName = device
        self._device = None
        self._categoricalRecognition = None
        self._dimensionalRecognition = None
        self._running = False
        self._facialExpression = face
        self._robot = robot

        self._modelCategorical = modelLoader.modelLoader(modelDictionary.CategoricaModel)
        self._modelDimensional = modelLoader.modelLoader(modelDictionary.DimensionalModel)
        self._graph = tf.get_default_graph()

        self._faceDetectionDelta = faceDetectionDelta
        self._imageProcessing = imageProcessingUtil.imageProcessingUtil(faceDetectionDelta)

        self._GUIController = GUIController.GUIController()


    def start(self, showGUI=True, faceTracking=False, mirrorEmotion=False):
        """
        Starts the emotion recognition

        :param showGUI: Whether or not the GUI should be displayed
        :type showGUI: bool
        :param mirrorEmotion: Whether or not the robot should mirror the detected emotion
        :type mirrorEmotion: bool
        """
        if self._running:
            logging.warning('Trying to start emotion recognition while already running')
            return
        self._mirrorEmotion = mirrorEmotion
        self._faceTracking = faceTracking
        self._trackingCounter = 0
        self._device = VideoDevice.fromDevice(self._deviceName)
        self._device.addCallback(self._callback)
        self._device.open()
        self._showGUI = showGUI
        self._running = True

    def stop(self):
        """
        Stops the emotion recognition
        """
        if not self._running:
            logging.warning("Trying to stop emotion recognition while it's not running")
            return
        self._device.close()
        self._device = None
        self._categoricalRecognition = None
        self._dimensionalRecognition = None
        cv2.destroyAllWindows()
        self._running = False

    def getDimensionalData(self):
        """
        Returns the dimensional data of the currently detected face (or None if there is none)

        :return: Arousal and Valence score (or None if no face detected)
        :rtype: dict
        """
        if not self._running:
            logging.warning('Dimensional data requested while emotion recognition not running')
            return None
        if self._dimensionalRecognition==None:
            logging.info("No face detected - Dimensional data will be 'None'")
            return None
        return dict(zip(self._modelDimensional.modelDictionary.classsesOrder, map(lambda x: float(float(x[0][0])*100), self._dimensionalRecognition)))

    def getCategoricalData(self):
        """
        Returns the categorical data of the currently detected face (or None if there is none)

        :return: Neutral, Happiness, Surprise, Sadness, Anger, Disgust, Fear and Contempt percentages (or None if no face detected)
        :rtype: dict
        """
        if not self._running:
            logging.warning('Categorical data requested while emotion recognition not running')
            return None
        if self._dimensionalRecognition==None:
            logging.info("No face detected - Categorical data will be 'None'")
            return None
        return dict(zip(self._modelCategorical.modelDictionary.classsesOrder, self._categoricalRecognition[0]))

    def getHighestMatchingEmotion(self):
        """
        Returns the name of the highest matching emotion for the currently detected face (or None if there is none)

        :return: Neutral, Happiness, Surprise, Sadness, Anger, Disgust, Fear or Contempt (or None if no face detected)
        :rtype: String
        """
        if self._categoricalRecognition is not None:
            max_index = numpy.argmax(self._categoricalRecognition[0])
            max_classname = self._modelCategorical.modelDictionary.classsesOrder[max_index]
            return self._modelCategorical.modelDictionary.classsesOrder[numpy.argmax(self._categoricalRecognition[0])].lower()
        return None


    def _callback(self, rval, frame):
        if frame is not None:
            facePoints, face = self._imageProcessing.detectFace(frame)

            if self._showGUI:
                image = numpy.zeros((self._finalImageSize[1], self._finalImageSize[0], 3), numpy.uint8)
                image[0:480, 0:640] = frame
                frame = image



            if not len(face) == 0:
                if self._faceTracking and self._trackingCounter == 0:
                    if self._robot is not None:
                        # (width - center_x)/width * FOV - FOV/2
                        angle_z = (640-facePoints[0].center().x)/640.0 * 60 - 60/2.0 # horizontal
                        angle_y = (480-facePoints[0].center().y)/480.0 * 50 - 50/2.0 # vertikal
                        self._robot.changeAngle("head_z", angle_z, 0.02)
                        self._robot.changeAngle("head_y", -angle_y, 0.02)
                    else:
                        logging.warning("No robot given on initialisation - skipping face tracking")
                if self._trackingCounter == self._faceDetectionDelta:
                    self._trackingCounter = 0
                else:
                    self._trackingCounter += 1
                face = self._imageProcessing.preProcess(face, self._faceSize)
                with self._graph.as_default():
                    self._categoricalRecognition = self._modelCategorical.classify(face)
                    self._dimensionalRecognition = self._modelDimensional.classify(face)

                if self._mirrorEmotion and self._facialExpression is not None:
                    self._facialExpression.sendFaceExpression(self.getHighestMatchingEmotion())

                if self._showGUI:
                    frame = self._GUIController.createDetectedFacGUI(frame,facePoints,self._modelCategorical.modelDictionary, self._categoricalRecognition)
                    frame = self._GUIController.createDimensionalEmotionGUI(self._dimensionalRecognition, frame, self._categoricalRecognition, self._modelCategorical.modelDictionary)
                    frame = self._GUIController.createCategoricalEmotionGUI(self._categoricalRecognition,frame,self._modelCategorical.modelDictionary, initialPosition=self._categoricalInitialPosition)
            else:
                self._categoricalRecognition = None
                self._dimensionalRecognition = None
                if self._faceTracking:
                    if self._robot is not None:
                        self._robot.setAngle("head_z", 0, 0.05)
                        self._robot.setAngle("head_y", 10, 0.05)
                    else:
                        logging.warning("No robot given on initialisation - skipping face tracking")

            if self._showGUI:
                # Display the resulting frame
                cv2.imshow('Visual Emotion Recognition',frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    #break
                    return
